[TOC]

第7章 深入客户端

深入挖掘Kafka的原理，从底层的概念构建Kafka的知识体系

# 7.1 分区分配策略

kafka提供了消费者客户端参数 partition.assignment.strategy 来设置消费者与主题之间的分区分配策略；默认情况下该值为 org.apache.kafka.clients.consumer.RangeAssignor，Kafka还提供了另外两种策略 RoundRibonAssignor (轮询)和 StickyAssignor (粘性)，可以设置多个分配策略，用逗号分隔；

## 7.1.1 RangeAssignor 分配策略

对于**每一个主题**，将该**消费者组**中订阅该主题的所有**消费者**按字典排序，然后为每个消费者划分固定的**分区范围**，如果不够平均，排序靠前的消费者会多分配一个分区；

**场景：**

两个主题t0,t1 ，各有3个分区t0p0,t0p1,t0p2,t1p0,t1p1,t1p2,消费者组group-demo内有2个消费者c0，c1，那么：

c0: t0p0,t0p1,t1p0,t1p2;

c1: t0p2,t1p2;

由此可见，这种情况下，会出现消息分配不均的情况，会出现消费者过载的情况

## 7.1.2 RoundRibonAssignor 分配策略

将消费者组内的所有消费者及消费者订阅的主题的所有分区按照字典排序，然后通过轮询的方式依次将分区分配给每个消费者；

如果消费者组内的**消费者**的**订阅信息**相同，那么分配策略是均匀的；

如果消费者的订阅信息不相同，那么分区分配就不是完全的均匀分配，有可能导致分区分配的不均匀；

**场景：**

3个主题t0,t1,t2，每个主题分别有1，2，3个分区；3个消费者c0,c1,c2，c0订阅t0，c1订阅t0和t1，c2订阅t0,t1,t2；分配策略为：

c0: t0p0

c1: t1p0,

c2: t1p1,t2p0,t2p1,t2p3



## 7.1.3 StickyAssignor 分配策略

保证两个目标：1.尽量保证分配的均匀；2.分区的分配尽可能保证和上次的相同；

第1条优先级最高；

在 7.1.2 场景中，为了保证第一条，此时的分配策略为：

c0: t0p0

c1: t1p0,t1p1

c2: t2p0,t2p1,t2p3



第2条，在消费者组内的消费者订阅的信息一致的时候，该策略初始分配和RoundRibon一样；只是在重分配的时候，该策略会保证某节点上本次的分配包含上次的分配；

**场景：**

3个消费者，4个主题，每个主题2个分区；分配策略为：

c0: t0p0,t1p1,t3p0

c1: t0p1,t2p0,t3p1

c2: t1p0,t2p1

当消费者c1下线时，分配策略为：

c0: t0p0,t1p1,**t2p0**,t3p0

c2: **t0p1,**t1p0,t2p1**,t3p1**



## 7.1.4 自定义分区分配策略

实现接口 PartitionAssignor接口，(Kafka 2.4以后使用接口 ConsumerPartitionAssignor)

```java
public interface PartitionAssignor {
	Subscription subscription(Set<String> topics);
    
    Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription> subscriptions);
    
    void onAssignment(Assignment assignment);
    
    default void onAssignment(Assignment assignment, int generation) {
        onAssignment(assignment);
    }
    
    String name();
    
    class Subscription {
        private final List<String> topics;
        private final ByteBuffer userData;
    }
    class Assignment {
        private final List<TopicPartition> partitions;
        private final ByteBuffer userData;
    }
    
}
```

Subscription 类表示消费者的订阅信息；topics和userData分别表示消费者的订阅主题列表和用户列表；

通过 subscription 方法设置消费者自身相关的Subscription信息，最终发送给leader节点通过在assign方法中使用；

assign方法是真正的分配方法，输入参数分别是集群的信息，和每个消费者的订阅信息；输出每个消费者的分配信息；

onAssignment是消费者收到分配信息后的回调方法，StickyAssignor类是通过该方法保存当前的分配策略；

**随机分配策略**

为每一个主题的分区，随机分配一个消费者消费；

```java
package org.example.shch.kafka.consumer;

import org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor;
import org.apache.kafka.common.TopicPartition;

import java.util.*;

/**
 * 随机 分区分配器
 *
 * @author shichao
 * @since 1.0.0
 * 2021/4/21 23:56
 */
public class RandomAssignor extends AbstractPartitionAssignor {
    @Override
    public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions) {
        Map<String, List<String>> consumersPerTopic = this.getConsumersPerTopic(subscriptions);

        Map<String, List<TopicPartition>> assignments = new HashMap<>();
        for (String consumerId : subscriptions.keySet()) {
            assignments.put(consumerId, new ArrayList<>());
        }
        // 为每个topic的分区，分配消费者
        consumersPerTopic.forEach((topic, consumerIds) -> {
            Integer partitionNum = partitionsPerTopic.get(topic);
            if (partitionNum == null) {
                return;
            }
            // 计算topic的所有分区信息
            List<TopicPartition> partitions = AbstractPartitionAssignor.partitions(topic, partitionNum);
            // 为每个分区随机分配一个消费者
            for (TopicPartition partition : partitions) {
                int rand = new Random().nextInt(consumerIds.size());
                String randomConsumerId = consumerIds.get(rand);
                assignments.get(randomConsumerId).add(partition);
            }
        });
        return assignments;
    }

    /**
     * 获取每个主题与其消费者的关系
     *
     * @param subscriptions 每个消费者的订阅信息
     * @return [topic:[consumer1,],]
     */
    private Map<String, List<String>> getConsumersPerTopic(Map<String, Subscription> subscriptions) {
        Map<String, List<String>> topicConsumersMap = new HashMap<>();
        subscriptions.forEach((consumerId, subscription) -> {
            for (String topic : subscription.topics()) {
                if (!topicConsumersMap.containsKey(topic)) {
                    topicConsumersMap.put(topic, new ArrayList<>());
                }
                topicConsumersMap.get(topic).add(consumerId);
            }
        });
        return topicConsumersMap;
    }

    @Override
    public String name() {
        return "random";
    }
}
```

消费者在使用的时候，需要配置属性：

```
properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RandomAssignor.class.getName());
```

之前曾说过，一个分区只能同时被一个消费者组的一个消费者消费；但是这个设定并不是绝对的，可以通过自定义分区分配策略将一个分区分配给所有消费者，即**组内广播**；

coding...

# 7.2 消费者协调器和组协调器

如果消费者客户端中配置了两个分配策略，以哪个为准？

如果有多个消费者，彼此所分配的分配策略并无完全相同，那么以哪个为准？

多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎么样？

这一切都是交给消费者协调器ConsumerCoordinator，和组协调器GroupCoordinator来完成的，它们之间使用一套组协调协议进行交互。



## 7.2.1 旧版本消费者客户端的问题

旧版消费者客户端是使用zookeeper的监听器来实现这些功能的。

每个消费者组\<group\>在zookeeper中维护一个 /consumers/\<group\>/ids 路径，路径下通过使用临时节点(consumerIdString)标识每个消费者;

与ids同级的还有两个节点，owners和offsets，owners记录了分区和消费者的对应关系，offsets记录此消费者组在分区中对应的消费位移；

每个broker，主题和分区在zookeeper中也都对应一个路径，/brokers/ids/\<id\>记录broker的host,port,及分配到该broker上的主题分区列表；

/brokers/topics/\<topic\> 记录所有分区信息，分区的leader副本，ISR集合等信息；

/brokers/topics/\<topic\>/partitions/\<partition\>/state 记录当前leader副本，leader_epoch等信息。



每个消费者在启动的时候，都会在 /consumers/\<group\>/ids和/brkers/ids路径上注册一个监听器，可以监听消费者组和Kafka集群的状态；

这种方式下，每个消费者对zookeeper的相关路径分别监听，当触发再均衡操作时，一个消费者组下的所有消费者会同时进行再均衡操作，而消费者之间不知道彼此操作的结果，这就可能导致Kafka工作在一个不正确的状态；像这种严重依赖zookeeper还会有如下两个问题：

1. 羊群效应，zookeeper 中一个被监听的节点发生变化，大量的watcher通知被发送到客户端，导致通知期间的其他操作延迟，也有可能发生类似的死锁的情况；
2. 脑裂问题，消费者再均衡时每个消费者都与zookeeper进行通信以判断消费者或broker变化的情况，由于zookeeper本身的特性，可能导致同一时刻各个消费者回去的状态不一致，这样会导致异常问题发生；



## 7.2.2 再均衡的原理

新版消费者客户端对此进行了重新设计，将全部消费组分成多个子集，每个消费者组自己**在服务端**对应一个 GroupCoordinator 对其进行管理，它是Kafka服务端中用于管理消费者组的组件；

而消费者客户端中的 ConsumerCoordintor组件负责与GroupCoordinator进行交互；

它们之间最重要的职责就是负责执行消费者再均衡的操作，如何会触发再均衡？

- 有新的消费者加入消费者组；
- 有消费者宕机下线；
- 有消费者主动退出消费者组；
- 消费者组对应的GroupCoordinator节点发生了变更；
- 消费者组内所订阅的任一主题或主题的分区数目发生变化；



再均衡的具体过程：

**第一阶段 FIND_COORDINATOR**

消费者需要确定它所属消费者组对应的GroupCoordinator所在的broker，并且创建与该broker相互通信的网络连接；

如果一开始消费者不知道这些信息，就需要向集群中的某个节点发送 **FindCoordinatorRequest请求**，查找对应的GroupCoordinator;

请求信息中会包含消费者组字段，groupId值

具体查找GroupCoordinator方式为，

1. 先根据消费者组的groupId的hash值计算__consumer_offsets中的分区编号，groupId.hashCode % groupMetadataTopicPartitionCount；
2. 寻找此分区的leader副本所在的broker节点，该broker节点即为GroupCoordinator；

消费者groupId的最终分区分配方案及组内消费位移的提交等信息都会发送给此GroupCoordinator；



**第二阶段 JOIN_GROUP**

消费者向GroupCoordinator发送  **JoinGroupRequest请求**，并处理相应；

请求信息中会携带，

1. group_id
2. session_timeout 对应消费者端参数 session.timeout.ms，默认h值10秒，即10秒内GroupCoordinator没收到该消费者心跳信息，就认为此消费者下线；
3. rebalance_timeout 对应消费端参数 max.poll.interval.ms，默认值为5分钟，表示当消费者组再均衡的时候，GroupCoordinator等待个消费者重新加入的最长等待时间；
4. member_id 表示GroupCoordinator分配给消费者的id表示，消费者第一次发送 JoinGroupRequest的时候，此字段为空；
5. group_protocols 为数组类型，为当前消费者配置的分区分配策略，即由参数 partition.assignment.strategy配置；

如果原有的消费者重新加入消费者组，那么在发送加入请求前会做如下事情：

1. 如果消费端配置 enable.auto.commit=true，那么在请求加入前需要向GroupCoordinator提交位移，这个过程是阻塞的；
2. 如果消费者添加了自定义的再均衡监听器(ConsumerRebalanceListener)，那么此时会先调用 onPartitionsRevoked()方法，在重新加入组之前试试自定义的规则逻辑，如清除状态，提交消费位移等；
3. 之前的心跳检测不需要了，在重新成功加入消费者组前需要禁止心跳检测的运作；

**选举消费组的leader**

GroupCoordinator需要为消费组内的消费者选举出一个消费者组的leader，选举算法很简单，如果当前组内还没有消费者，那么第一个加入的就是leader；如果leader消费者下线了，那么所有的消费者排序取第一个；

**选举分区分配策略**

GroupCoordinator会根据每个消费者发送的支持的分区分配策略，进行收集选出支持做多的一个作为具体的分区分配策略，如果某个消费者不支持该策略，就会报错；



上述之后，GroupCoordinator需要发送JoinGroupResponse给各个消费者，只有leader的响应中会包含字段 members值，表示各消费者的信息；

同时具体的分区分配策略也在响应中体现；



**第三阶段  SYNC_GROUP**

leader消费者会从选举出来的分区分配策略来实施具体的方案，之后同步给各消费者；leader消费者并不是直接与其余消费者通信，而是通过GroupCoordinator负责同步分配方案；

在同步节点，各消费者回想GroupCoordinator发送 **SyncGroupRequest** 的请求，来同步分配方案；

其中leader消费者的请求中会携带具体的分配方案；

GroupCoordinator收到分区分配方案后，会首先连同整个消费组的元数据信息一起才能入Kafka的__consumer_offset主题中，最后发送响应给各个消费者各自所属的分配方案；



**第四阶段 HEARTBEAT**

进入该阶段后，消费者组中的消费者就处于正常工作状态，在正式消费之前，需要通过 OffsetFetchRequest 请求向GroupCoordinaor请求上次提交的位移，并从此处开始消费；

心跳是一个独立的线程，如果消费者停止发送心跳的时间足够长，则整个会话会被认为过期；心跳间隔由参数 heartbeat.interval.ms，默认3秒，该值不能超过 session.timeout.ms的三分之一；

如果一个消费发生崩溃，停止读取消息，那么GroupCoordinator会等待 一段时间，确认该消费者已经死亡后才会触发再均衡；这段时间由参数 session.timeout.ms控制，该参数的配置需要在broker端的参数 group.min.session.timeout.ms(默认6秒)和group.max.session.timeout.ms(默认5分钟)允许的范围。

max.poll.interval.ms 参数用来指定使用消费者组管理时，poll方法调用之间的最大延迟，如果超时未执行poll方法，那么消费者被视为失败，并且重新平衡，将分区重新分配给别的成员。	





# 7.3 __consumer_offset 剖析

消费者位移的提交是通过使用 **OffsetCommitRequest** 请求实现的；

请求中有字段topics，数组类型，每个元素有字段topic和partitions字段；

partitions字段，数组类型，有partition和offset字段，表示具体分区的偏移量值；

可以通过 kafka-topic-consumer.sh 脚本查看 __consumer_offsets中的内容，不过要提前设定formatter参数



# 7.4 事务





## 7.4.2 幂等

对接口的多次调用所产生的结果和调用一次是一致的；

Kafka生产者在写入消息时，如果发生重试发送，有可能写入重复的消息，而使用Kafka的幂等性可以避免这种情况；

设置参数：enable.idempotence 为true即可；

**原理：**

kafka 为生产者引入了produceID(PID)和序列号(sequence)两个概念，生产者每发送一条消息都会将<PID，分区>的序列号的值加1；

broker端同时也会为每个<PID，分区>维护一个序列号；对收到的每条消息，只有当消息的序列号(seq_new)值比broker端的序列号(seq_old)值大1才可以；如果seq_new<seq_old+1，那么broker会将其丢弃；如果seq_new>seq_old+1 会报错，说明发生了消息丢失现象；

**应用场景**

Kafka的幂等性只是保证单个会话单个分区的幂等；



## 7.4.3 事务

幂等性不能跨分区，而事务可以弥补这个缺陷；事务可以保证多个分区写入操作的原子性，多个操作要么全部成功要么全部失败；

设置生产端配置transactional.id为具体的id值；

1.  具有相同transactionId的新的生产者实例创建工作时，旧的将不能使用；

KafkaProducer 提供事务相关的5个方法

- initTransactions();
- beginTransaction();
- sendOffsetsTransaction<Map<TopicPartition,OffsetAndMetadata>offsets,String consumerGroupId)
- commitTransaction()
- abortTransacion()

在消费端有个参数 isolation.level设置隔离级别，默认值是 read_uncommited，意味着消费者可以读到未提交的事务；可以设置为 read_commited，可以使得消费者看不到；

**实现原理：**

引入了事务协调器来负责处理事务，每一个生产者都会被指派到一个特定的TransactionCoordinator，TransactionCoordinator会将事务状态持久化内部主题 __transaction_state中

1. 查找TransactionCoordinator：发送FindCoordinatorRequest请求；broker根据生产者的transactionId值，对__transaction_state的分区数目进行模运算，将计算出的分区号对应的leader副本的broker节点作为TransactionCoordinator；
2. 保存PID，发送InitProducerIdRequest请求；
3. 开启事务，生产者端调用beginTransaction()方法，只有生产者发送第一条事务消息的时候，TransactionCoordinator才会任务事务已经开始；
4. Consume-Transform-Produce 消费-转换处理-生产
   1. AddPartitionToTxnRequest 生产者把发送的主题信息告诉TransactionCoordinator，协调器会把信息持久化到__transaction_state上；
   2. ProduceRequest 生产者正常的发送消息，区别在于该请求中会写到和事务相关的PID等信息；
   3. AddOffsetToTxnRequest 调用提交位移的方法时，需要把消费者提交的位移信息告诉TransactionCoordinator，协调器保存到__transaction_state中持久化；
   4. TxnOffsetCommitRequest 协调器把位移信息存储到到对应主题__offset_commit下；
5. 提交或终止事务；
   1. EndTxnRequest 生产者告诉协调器结束事务，通过调用commitTransaction()或abortTransaction()实现；协调器会把PREPARE_COMMIT或PREPARE_ABORT信息写入__transaction_state中；
   2. WriteTxnMarkersRequest 协调器会将当前的控制信息发送到每一个__transaction_state分区的leader节点上，存储当前事务的控制信息；
   3. 写入最终的COMPLETE_COMMIT或COMPLETE_ABORT信息到__transaction_state中，同时可以删除该主题中和当前事务相关的消息；这里的删除采用的是日志压缩，只需将消息设置为墓碑消息即可；

有哪些删除策略？什么是墓碑消息？

日志清理策略有两个：日志删除和日志压缩(log Compaction)，

日志删除指的是可以设置一定的规则比如时间，大小，logStartOffset等，使得不符合要求的日志片段清理掉；

日志压缩是将相同的key的消息合并，最后只保存最新的；

墓碑消息是值消息的value为空；







# 7.5 总结



